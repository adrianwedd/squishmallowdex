#!/usr/bin/env python3
"""
‚ú® Squishmallowdex ‚ú®
Build a local, searchable Squishmallow table from the Squishmallows Wiki (Fandom).

This is YOUR personal Pok√©dex... but for Squishmallows!
Run this script to catch 'em all and build an offline collection you can browse anytime.

Personal/educational use only. Be polite: cache and delay requests.
This script is written to be friendly for learning and tinkering.
"""

from __future__ import annotations

import argparse
import base64
import csv
import hashlib
import json
import os
import random
import re
import sys
import time
from collections import Counter
from dataclasses import dataclass, field
from html import escape
from typing import TextIO
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üé® FUN OUTPUT SYSTEM - Makes the console exciting!
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def skipped_file_path(progress_file: str) -> str:
    """
    Derive the skipped-URLs file path from the progress file path.
    Works correctly regardless of file extension.
    """
    base, ext = os.path.splitext(progress_file)
    return f"{base}_skipped{ext or '.txt'}"


def load_ansi_art(filepath: str) -> str | None:
    """
    Load ANSI art from a file.
    Returns the content if the file exists, None otherwise.
    """
    if not os.path.exists(filepath):
        return None
    try:
        with open(filepath, "r", encoding="utf-8", errors="replace") as f:
            return f.read()
    except Exception:
        return None


# Path to the phoenix ANSI art (relative to script location)
PHOENIX_ART_PATH = os.path.join(os.path.dirname(__file__) or ".", "phoenix.t.utf.ans")

# Path to the logo image for HTML output
LOGO_PATH = os.path.join(os.path.dirname(__file__) or ".", "squishmallowdex.png")


@dataclass
class AdventureLog:
    """Tracks stats and handles fun, educational output for young collectors."""

    verbose: int = 0  # 0=normal, 1=verbose (-v), 2=debug (-vv)
    adventure_mode: bool = True  # Default: adventure mode ON for maximum fun!
    use_color: bool = True
    quiet: bool = False  # Minimal output mode
    show_phoenix_art: bool = False  # Show the phoenix celebration at the end
    start_time: float = field(default_factory=time.time)
    log_file: str = "squishmallow.log"  # File to write logs to
    _file_handle: TextIO | None = field(default=None, repr=False)

    # Stats we track during the run
    new_catches: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    skipped: int = 0
    errors: int = 0
    types_seen: Counter = field(default_factory=Counter)
    colors_seen: Counter = field(default_factory=Counter)
    squads_seen: Counter = field(default_factory=Counter)
    sizes_seen: Counter = field(default_factory=Counter)
    years_seen: Counter = field(default_factory=Counter)
    letters_seen: set[str] = field(default_factory=set)
    names_collected: list[str] = field(default_factory=list)

    # Fun celebration milestones
    MILESTONES = [10, 25, 50, 100, 150, 200, 250, 300, 400, 500, 750, 1000]

    # Educational facts shown during download
    DID_YOU_KNOW = [
        # Internet & Networking
        "üí° The internet sends data in tiny packets, like digital letters!",
        "üí° Servers are big computers that store websites for everyone to visit!",
        "üí° URLs are like street addresses for websites!",
        "üí° HTTP stands for HyperText Transfer Protocol - it's how browsers talk to servers!",
        "üí° Your computer's 'User-Agent' tells websites what browser you're using!",
        "üí° WiFi signals are invisible radio waves bouncing around your house!",
        "üí° Data can travel around the world in less than a second!",
        "üí° The first website ever made is still online - it was created in 1991!",
        "üí° Undersea cables carry 99% of international internet traffic!",
        "üí° A megabyte is about 1 million characters - like a really long book!",
        # Programming & Code
        "üí° Python was named after Monty Python, not the snake! üêç",
        "üí° HTML is the language websites speak - it stands for HyperText Markup Language!",
        "üí° JSON is a way computers share information - it's like a recipe card!",
        "üí° A 'bug' got its name when a real moth got stuck in an early computer!",
        "üí° Programmers use 'loops' to repeat things - like singing a chorus!",
        "üí° Variables are like labeled boxes that store information!",
        "üí° An 'algorithm' is just a fancy word for step-by-step instructions!",
        "üí° The first programmer ever was Ada Lovelace, in the 1840s!",
        "üí° Computers only understand 1s and 0s - called binary code!",
        "üí° Emojis are actually a type of font, just like letters! üòä",
        # This Script
        "üí° A 'cache' is like a memory box - we save pages so we don't download twice!",
        "üí° Parsing means reading through data to find the important parts!",
        "üí° A hash turns any text into a unique code - like a fingerprint for data!",
        "üí° CSV files are spreadsheets that any program can read!",
        "üí° Timeouts prevent your program from waiting forever if a server is slow!",
        "üí° Web scraping is like being a detective, finding clues hidden in web pages!",
        "üí° Rate limiting means being polite - we wait between requests so servers aren't overwhelmed!",
        "üí° This script saves your progress so you can stop and resume anytime!",
        "üí° Each Squishmallow page gets a unique filename based on its URL!",
        "üí° The HTML file works offline - no internet needed to browse your collection!",
        # Fun Tech Facts
        "üí° The first computer mouse was made of wood!",
        "üí° A 'gigabyte' could store about 300 songs!",
        "üí° The @ symbol is over 500 years old!",
        "üí° The first video game was created in 1958!",
        "üí° Robots on Mars run on code similar to what powers this script!",
        "üí° Some computers can do billions of calculations per second!",
        "üí° QR codes were invented to track car parts in factories!",
        "üí° The cloud is really just other people's computers storing your stuff!",
        "üí° Ctrl+Z for undo was invented in 1976!",
        "üí° The caps lock key exists because early keyboards couldn't do lowercase!",
        # Gentle Reminders & Self-Care
        "üíß Hey! Have you had some water recently? Your brain works better hydrated!",
        "üíß Water break! Even computers need cooling - and so do you!",
        "üíß Quick reminder: grab a drink of water! Hydration = happy brain!",
        "üåü Stretch break! Roll your shoulders and take a deep breath!",
        "üåü How's your posture? Sit up tall like a proud Squishmallow!",
        "üåü Blink a few times! Your eyes are working hard!",
        "üåü Take three deep breaths - you're doing great!",
        "üíú Hey, have you told Mum she's beautiful today? Go tell her!",
        "üíú Reminder: Tell your closest sibling they're awesome!",
        "üíú When did you last hug someone you love? Hugs are free!",
        "üíú Your family loves you! Maybe go tell them you love them too!",
        "üíú Smile at someone today - smiles are contagious!",
        "üíú You could make someone's day - tell them something you like about them!",
        "üéÄ You're doing amazing! Collecting Squishmallows AND learning code!",
        "üéÄ Fun fact: YOU are the rarest collector of all - there's only one you!",
        "üéÄ Taking breaks is smart, not lazy. Rest when you need to!",
        "üéÄ Mistakes help us learn - every coder makes them!",
        "üéÄ Being kind to yourself is just as important as being kind to others!",
        "üçé Have you eaten something today? Brains need fuel!",
        "üçé A healthy snack might be nice right about now!",
        # Squishmallow Collecting Tips
        "üè∑Ô∏è TIP: Check the tag! Collector Numbers help identify rare editions!",
        "üè∑Ô∏è TIP: Keep your tags if you want to know the Squishdate later!",
        "üè∑Ô∏è TIP: The bio poem on the tag tells you their personality!",
        "üîç TIP: Use the search in your Squishmallowdex to find squish by color!",
        "üîç TIP: Search by Squad to see all your Halloween or Valentine squish!",
        "üîç TIP: You can search by Type to find all your unicorns or cats!",
        "üì∏ TIP: Take photos of your collection - it's fun to see it grow!",
        "üì∏ TIP: A group photo of your squish makes a great phone wallpaper!",
        "üéÅ TIP: Trading with friends is a fun way to grow your collection!",
        "üßπ TIP: Spot clean your Squishmallows with mild soap and water!",
        "üßπ TIP: Let your squish air dry completely before cuddling again!",
        "üßπ TIP: Some Squishmallows can go in a pillowcase in the washing machine!",
        "üõèÔ∏è TIP: Squishmallows make great pillows for reading or watching shows!",
        "üõèÔ∏è TIP: Stack your squish to make a cozy reading nook!",
        "üõèÔ∏è TIP: A Squishmallow under your arm helps you sleep on your side!",
        "üì¶ TIP: Store squish in a breathable bag or bin - not airtight plastic!",
        "üì¶ TIP: Rotate which squish are on display to keep things fresh!",
        "üì¶ TIP: A hanging net in the corner is great for displaying squish!",
        "üåà TIP: Sort your collection by color for a rainbow display!",
        "üåà TIP: Group squish by Squad for themed displays!",
        "üåà TIP: Smaller sizes are great for backpacks and travel!",
        "‚≠ê TIP: Clip-on Squishmallows are perfect for bags and keychains!",
        "‚≠ê TIP: Flip-a-Mallows are two squish in one - flip them inside out!",
        "‚≠ê TIP: HugMees are extra long for hugging!",
        "‚≠ê TIP: Stackables are flat on the bottom so they don't roll away!",
        "‚≠ê TIP: Squishville are tiny collectible Squishmallows with playsets!",
        "‚ù§Ô∏è TIP: Every Squishmallow has a name and birthday - learn your faves!",
        "‚ù§Ô∏è TIP: Some squish have the same name but different looks - variants!",
        "‚ù§Ô∏è TIP: Your first Squishmallow is special - remember which one it was?",
        "‚ù§Ô∏è TIP: It's okay to have favourites - they all still love you!",
        # Squishmallowdex Script Tips
        "üñ•Ô∏è SQUISHMALLOWDEX: Catch 10 new squish ‚Üí python3 squishmallowdex.py --limit 10",
        "üñ•Ô∏è SQUISHMALLOWDEX: See your stats ‚Üí python3 squishmallowdex.py --stats-only",
        "üñ•Ô∏è SQUISHMALLOWDEX: Faster run (skip images) ‚Üí python3 squishmallowdex.py --no-download-images",
        "üñ•Ô∏è SQUISHMALLOWDEX: Press Ctrl+C anytime to stop - your progress is saved!",
        "üñ•Ô∏è SQUISHMALLOWDEX: Quiet mode ‚Üí python3 squishmallowdex.py --quiet",
        "üñ•Ô∏è SQUISHMALLOWDEX: Maximum detail ‚Üí python3 squishmallowdex.py -vv",
        "üñ•Ô∏è SQUISHMALLOWDEX: Force re-download ‚Üí python3 squishmallowdex.py --refresh",
        "üñ•Ô∏è SQUISHMALLOWDEX: Check squishmallow.log for a full record of each run!",
        "üñ•Ô∏è SQUISHMALLOWDEX: Open squishmallowdex.html in a browser to search your collection!",
        "üñ•Ô∏è SQUISHMALLOWDEX: Open squishmallowdex.csv in Excel or Google Sheets!",
        "üñ•Ô∏è SQUISHMALLOWDEX: See all options ‚Üí python3 squishmallowdex.py --help",
        "üñ•Ô∏è SQUISHMALLOWDEX: The cache_html folder makes reruns super fast!",
        "üñ•Ô∏è SQUISHMALLOWDEX: Skip fun facts ‚Üí python3 squishmallowdex.py --no-adventure",
        "üñ•Ô∏è SQUISHMALLOWDEX: Catch 'em all! ‚Üí python3 squishmallowdex.py --limit 0",
        "üñ•Ô∏è SQUISHMALLOWDEX: Combine options ‚Üí python3 squishmallowdex.py --limit 50 --quiet",
        # Terminal Tips
        "üíª TERMINAL: Use 'cd folder' to change directory - cd = change directory!",
        "üíª TERMINAL: Type 'ls' to list files in your current folder!",
        "üíª TERMINAL: Use 'cat file.txt' to display a file's contents!",
        "üíª TERMINAL: Try 'head file.txt' to see just the first 10 lines!",
        "üíª TERMINAL: Use 'tail file.txt' to see the last 10 lines!",
        "üíª TERMINAL: 'grep word file.txt' searches for 'word' in a file!",
        "üíª TERMINAL: Use 'grep -i' for case-insensitive search!",
        "üíª TERMINAL: The pipe | sends output to another command!",
        "üíª TERMINAL: Try 'cat file.txt | grep word' to search file contents!",
        "üíª TERMINAL: Use 'wc -l file.txt' to count lines in a file!",
        "üíª TERMINAL: Press Tab to autocomplete file and folder names!",
        "üíª TERMINAL: Press Up arrow to recall your last command!",
        "üíª TERMINAL: Use 'pwd' to see what folder you're in (print working directory)!",
        "üíª TERMINAL: 'mkdir newfolder' creates a new folder!",
        "üíª TERMINAL: 'cp file1 file2' copies a file!",
        "üíª TERMINAL: 'mv file1 file2' moves or renames a file!",
        "üíª TERMINAL: Use 'clear' to clear your terminal screen!",
        "üíª TERMINAL: 'python3 script.py' runs a Python script!",
        "üíª TERMINAL: Use 'less file.txt' to scroll through a big file!",
        "üíª TERMINAL: Press 'q' to quit less, man pages, and other viewers!",
    ]

    CATCH_MESSAGES = [
        # Classic catches
        "‚ú® CAUGHT! {name} joined your Squishmallowdex!",
        "üéØ GOT ONE! {name} is now in your collection!",
        "‚≠ê AWESOME! You found {name}!",
        "üíñ SQUISH! {name} has been added!",
        "üåü NICE CATCH! {name} is yours!",
        "üí´ WOOHOO! {name} collected!",
        "üé™ AMAZING! {name} joins the squad!",
        "üé® BEAUTIFUL! {name} is now catalogued!",
        # Excitement
        "üéâ YES! {name} is in the collection!",
        "üåà WONDERFUL! {name} found a new home!",
        "üíñ ADORABLE! {name} joins the family!",
        "üéä HOORAY! Say hello to {name}!",
        "ü•≥ PARTY TIME! {name} has arrived!",
        "‚ú® SPARKLY! {name} is now yours!",
        "üå∏ LOVELY! {name} joins the crew!",
        "üíï SWEET! {name} collected successfully!",
        # Action-packed
        "üöÄ ZOOM! {name} blasts into your collection!",
        "‚ö° ZAP! {name} captured!",
        "üí• BOOM! {name} is yours now!",
        "üéÆ LEVEL UP! {name} unlocked!",
        "üèÜ VICTORY! {name} acquired!",
        "üéØ BULLSEYE! {name} added!",
        "üî• ON FIRE! {name} collected!",
        "üíé JACKPOT! {name} is in!",
        # Cute & Cozy
        "üßÅ SWEET! {name} joins the cuddle pile!",
        "‚òÅÔ∏è FLUFFY! {name} floats into your collection!",
        "üåô DREAMY! {name} found!",
        "üç¨ SUGAR! {name} is collected!",
        "üéÄ PRECIOUS! {name} joins you!",
        "üå∫ PRETTY! {name} catalogued!",
        "ü¶ã FLUTTER! {name} lands in your dex!",
        "üç© SWEET CATCH! {name} is yours!",
        # Adventurous
        "üó∫Ô∏è DISCOVERED! {name} joins the adventure!",
        "üîç FOUND IT! {name} was hiding here!",
        "üèïÔ∏è ADVENTURE! {name} joins the expedition!",
        "üåä SPLASH! {name} dives into your collection!",
        "üèîÔ∏è SUMMIT! {name} conquered!",
        "üåø WILD! {name} discovered in the wild!",
        "üîÆ MYSTICAL! {name} appears!",
        "üóùÔ∏è UNLOCKED! {name} joins the vault!",
        # Celebratory
        "üéµ HARMONY! {name} joins the chorus!",
        "üé≠ SPECTACULAR! {name} takes the stage!",
        "üé¨ ACTION! {name} enters the scene!",
        "üì∏ SNAPSHOT! {name} captured!",
        "üèÖ CHAMPION! {name} wins a spot!",
        "üëë ROYAL! {name} crowned and collected!",
        "üéÅ SURPRISE! {name} unwrapped!",
        "üå† STELLAR! {name} shoots into your dex!",
        # Quirky
        "ü¶Ñ MAGICAL! {name} gallops in!",
        "üêæ PAWSOME! {name} joins the pack!",
        "üéà POP! {name} floats into the collection!",
        "üçÄ LUCKY! {name} found!",
    ]

    MILESTONE_MESSAGES = [
        "üéâ WOW! You've collected {n} Squishmallows! You're a SUPER collector!",
        "üèÜ INCREDIBLE! {n} Squishmallows caught! Keep going!",
        "üéä AMAZING! {n} in your Squishmallowdex! You're on fire!",
        "‚≠ê STELLAR! {n} Squishmallows! This collection is LEGENDARY!",
    ]

    # ANSI color codes (disabled if use_color=False or terminal doesn't support it)
    COLORS = {
        "reset": "\033[0m",
        "bold": "\033[1m",
        "dim": "\033[2m",
        "pink": "\033[38;5;213m",
        "purple": "\033[38;5;141m",
        "blue": "\033[38;5;75m",
        "cyan": "\033[38;5;87m",
        "green": "\033[38;5;120m",
        "yellow": "\033[38;5;228m",
        "orange": "\033[38;5;215m",
        "red": "\033[38;5;203m",
        "gray": "\033[38;5;245m",
    }

    def _c(self, color: str, text: str) -> str:
        """Apply color if colors are enabled."""
        if not self.use_color:
            return text
        return f"{self.COLORS.get(color, '')}{text}{self.COLORS['reset']}"

    def _strip_ansi(self, text: str) -> str:
        """Remove ANSI color codes for file logging."""
        return re.sub(r'\033\[[0-9;]*m', '', text)

    def _log_to_file(self, text: str) -> None:
        """Write a line to the log file (without colors)."""
        if self._file_handle:
            clean = self._strip_ansi(text)
            self._file_handle.write(clean + "\n")
            self._file_handle.flush()

    def _print(self, text: str, **kwargs) -> None:
        """Print to console AND log to file."""
        print(text, **kwargs)
        # Don't log carriage-return lines (progress bars) to file
        if kwargs.get("end") != "\r":
            self._log_to_file(text)

    def open_log(self) -> None:
        """Open the log file for writing, creating parent directories if needed."""
        # Ensure parent directory exists
        log_dir = os.path.dirname(self.log_file)
        if log_dir:
            os.makedirs(log_dir, exist_ok=True)

        self._file_handle = open(self.log_file, "a", encoding="utf-8")
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        self._file_handle.write(f"\n{'=' * 70}\n")
        self._file_handle.write(f"Squishmallowdex run started at {timestamp}\n")
        self._file_handle.write(f"{'=' * 70}\n\n")
        self._file_handle.flush()

    def close_log(self) -> None:
        """Close the log file."""
        if self._file_handle:
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
            self._file_handle.write(f"\nRun completed at {timestamp}\n")
            self._file_handle.close()
            self._file_handle = None

    def banner(self) -> None:
        """Print the startup banner (suppressed in quiet mode)."""
        if self.quiet:
            return
        banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                             ‚ïë
‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó             ‚ïë
‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë             ‚ïë
‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë  MALLOWDEX  ‚ïë
‚ïë   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñÑ‚ñÑ ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë             ‚ïë
‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë  v0.1       ‚ïë
‚ïë   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ñÄ‚ñÄ‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù             ‚ïë
‚ïë                                                             ‚ïë
‚ïë     üåü Gotta Squish 'Em All! Your Offline Collection üåü     ‚ïë
‚ïë                                                             ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
        self._print(self._c("cyan", banner))

    def info(self, msg: str, level: int = 0) -> None:
        """Print info message if verbosity is high enough (suppressed in quiet mode)."""
        if self.quiet:
            return
        if self.verbose >= level:
            self._print(self._c("cyan", f"  ‚ÑπÔ∏è  {msg}"))

    def debug(self, msg: str) -> None:
        """Print debug message (only in super verbose mode, suppressed in quiet mode)."""
        if self.quiet:
            return
        if self.verbose >= 2:
            self._print(self._c("gray", f"     üîç {msg}"))

    def step(self, msg: str) -> None:
        """Print a major step (suppressed in quiet mode)."""
        if self.quiet:
            return
        self._print("")
        self._print(self._c("purple", f"  üöÄ {msg}"))
        self._print("")

    def catch(self, name: str, total_count: int) -> None:
        """Celebrate catching a new Squishmallow!"""
        self.new_catches += 1

        if self.quiet:
            return

        msg = random.choice(self.CATCH_MESSAGES).format(name=name)
        self._print(self._c("green", f"  {msg}"))

        # Check for milestones
        if total_count in self.MILESTONES:
            self._celebrate_milestone(total_count)

        # Occasionally show educational facts in adventure mode
        if self.adventure_mode and random.random() < 0.15:
            fact = random.choice(self.DID_YOU_KNOW)
            self._print(self._c("blue", f"     {fact}"))

    def _celebrate_milestone(self, n: int) -> None:
        """Big celebration for hitting a milestone!"""
        msg = random.choice(self.MILESTONE_MESSAGES).format(n=n)
        border = "üéâ" * 20
        self._print("")
        self._print(self._c("yellow", f"  {border}"))
        self._print(self._c("yellow", f"  {msg}"))
        self._print(self._c("yellow", f"  {border}"))
        self._print("")

    def skip(self, name: str, reason: str = "") -> None:
        """Note when we skip a page."""
        self.skipped += 1
        if self.quiet:
            return
        if self.verbose >= 1:
            reason_str = f" ({reason})" if reason else ""
            self._print(self._c("gray", f"  ‚è≠Ô∏è  Skipped: {name}{reason_str}"))

    def cache_hit(self, what: str) -> None:
        """Note a cache hit (verbose mode)."""
        self.cache_hits += 1
        if self.quiet:
            return
        if self.verbose >= 2:
            self._print(self._c("gray", f"     üíæ Cache hit: {what}"))

    def cache_miss(self, what: str) -> None:
        """Note a cache miss (verbose mode)."""
        self.cache_misses += 1
        if self.quiet:
            return
        if self.verbose >= 1:
            self._print(self._c("dim", f"     üåê Downloading: {what}"))

    def error(self, msg: str) -> None:
        """Print an error."""
        self.errors += 1
        self._print(self._c("red", f"  ‚ùå ERROR: {msg}"))

    def warn(self, msg: str) -> None:
        """Print a warning."""
        self._print(self._c("orange", f"  ‚ö†Ô∏è  {msg}"))

    def track_squish(self, row: dict) -> None:
        """Track stats about a collected Squishmallow."""
        if row.get("Type"):
            self.types_seen[row["Type"]] += 1
        if row.get("Color"):
            self.colors_seen[row["Color"]] += 1
        if row.get("Squad"):
            self.squads_seen[row["Squad"]] += 1
        if row.get("Size(s)"):
            # Sizes can be comma-separated, count each
            for size in row["Size(s)"].split(","):
                size = size.strip()
                if size:
                    self.sizes_seen[size] += 1
        if row.get("Year"):
            self.years_seen[row["Year"]] += 1
        if row.get("Name"):
            name = row["Name"]
            self.names_collected.append(name)
            if name and name[0].upper().isalpha():
                self.letters_seen.add(name[0].upper())

    def show_phoenix(self) -> None:
        """Display the epic phoenix ANSI art as a celebration (suppressed in quiet mode)."""
        if self.quiet:
            return
        art = load_ansi_art(PHOENIX_ART_PATH)
        if art:
            self._print("\n")
            # Print directly without color wrapping since it has its own ANSI codes
            # Note: This looks best in a color-capable terminal!
            print(art)
            self._log_to_file(art)  # Capture actual art in log file
            self._print(self._c("yellow", "    üî• THE PHOENIX RISES! YOUR COLLECTION IS LEGENDARY! üî•\n"))

    def summary(self, total_rows: int, existing_count: int, total_available: int = 0,
                 csv_path: str = "", html_path: str = "") -> None:
        """Print the epic summary at the end!"""
        elapsed = time.time() - self.start_time
        mins, secs = divmod(int(elapsed), 60)

        # Track if we should show phoenix at the end (easter egg for collecting ALL)
        collected_all = total_available > 0 and total_rows >= total_available

        self._print("")
        self._print(self._c("pink", "    SQUISHMALLOWDEX BATCH COMPLETE!"))
        self._print("")

        # ‚îÄ‚îÄ‚îÄ Run Stats ‚îÄ‚îÄ‚îÄ
        self._print(self._c("purple", "    ‚îÄ‚îÄ‚îÄ Run Stats ‚îÄ‚îÄ‚îÄ"))
        self._print(f"    Total collected:      {self._c('bold', str(total_rows))}")
        self._print(f"    New this run:         {self._c('green', str(self.new_catches))}")
        self._print(f"    Already had:          {existing_count}")
        self._print(f"    Skipped:              {self.skipped}")
        if self.errors:
            self._print(f"    Errors:               {self._c('red', str(self.errors))}")
        self._print(f"    Time taken:           {mins}m {secs}s")

        if self.verbose >= 1:
            self._print(f"    Cache hits:           {self.cache_hits}")
            self._print(f"    Downloads:            {self.cache_misses}")

        # ‚îÄ‚îÄ‚îÄ Completion Progress ‚îÄ‚îÄ‚îÄ
        if total_available > 0:
            self._print("")
            self._print(self._c("purple", "    ‚îÄ‚îÄ‚îÄ Completion Progress ‚îÄ‚îÄ‚îÄ"))
            pct = (total_rows / total_available) * 100
            remaining = total_available - total_rows
            self._print(f"    Progress:             {pct:.1f}% complete!")
            if remaining > 0:
                self._print(f"    Remaining:            {self._c('yellow', f'{remaining} more to catch em all!')}")
            else:
                self._print(self._c("green", "    üéâ YOU CAUGHT 'EM ALL! üéâ"))

        # ‚îÄ‚îÄ‚îÄ Collection Diversity ‚îÄ‚îÄ‚îÄ
        self._print("")
        self._print(self._c("purple", "    ‚îÄ‚îÄ‚îÄ Collection Diversity ‚îÄ‚îÄ‚îÄ"))
        self._print(f"    Unique types:         {len(self.types_seen)}")
        self._print(f"    Unique colors:        {len(self.colors_seen)}")
        self._print(f"    Unique squads:        {len(self.squads_seen)}")

        # Alphabet coverage
        if self.letters_seen:
            all_letters = set("ABCDEFGHIJKLMNOPQRSTUVWXYZ")
            missing = all_letters - self.letters_seen
            coverage = f"{len(self.letters_seen)}/26 letters"
            if missing and len(missing) <= 5:
                coverage += f" (missing: {', '.join(sorted(missing))})"
            elif not missing:
                coverage += " - COMPLETE! üéâ"
            self._print(f"    Alphabet coverage:    {coverage}")

        # ‚îÄ‚îÄ‚îÄ Types & Squads ‚îÄ‚îÄ‚îÄ
        self._print("")
        self._print(self._c("purple", "    ‚îÄ‚îÄ‚îÄ Types & Squads ‚îÄ‚îÄ‚îÄ"))
        if self.types_seen:
            top_type = self.types_seen.most_common(1)[0]
            self._print(f"    Most common type:     {top_type[0]} ({top_type[1]})")
            # Unicorns - types with only 1
            unicorns = [t for t, c in self.types_seen.items() if c == 1]
            if unicorns:
                self._print(f"    Rare types (only 1):  {len(unicorns)} unique types!")

        if self.colors_seen:
            top_color = self.colors_seen.most_common(1)[0]
            rarest_color = self.colors_seen.most_common()[-1]
            self._print(f"    Most common color:    {top_color[0]} ({top_color[1]})")
            self._print(f"    Rarest color:         {rarest_color[0]} ({rarest_color[1]})")

        if self.squads_seen:
            top_squad = self.squads_seen.most_common(1)[0]
            smallest_squad = self.squads_seen.most_common()[-1]
            self._print(f"    Biggest squad:        {top_squad[0]} ({top_squad[1]})")
            self._print(f"    Smallest squad:       {smallest_squad[0]} ({smallest_squad[1]})")

        # ‚îÄ‚îÄ‚îÄ Sizes ‚îÄ‚îÄ‚îÄ
        if self.sizes_seen:
            self._print("")
            self._print(self._c("purple", "    ‚îÄ‚îÄ‚îÄ Sizes ‚îÄ‚îÄ‚îÄ"))
            top_size = self.sizes_seen.most_common(1)[0]
            self._print(f"    Most common size:     {top_size[0]} ({top_size[1]})")
            self._print(f"    Size variety:         {len(self.sizes_seen)} different sizes")

        # ‚îÄ‚îÄ‚îÄ Years & Dates ‚îÄ‚îÄ‚îÄ
        if self.years_seen:
            self._print("")
            self._print(self._c("purple", "    ‚îÄ‚îÄ‚îÄ Years & Dates ‚îÄ‚îÄ‚îÄ"))
            years = sorted([y for y in self.years_seen.keys() if y])
            if years:
                self._print(f"    Year range:           {years[0]} - {years[-1]}")
            top_year = self.years_seen.most_common(1)[0]
            self._print(f"    Most common year:     {top_year[0]} ({top_year[1]})")


        # ‚îÄ‚îÄ‚îÄ Name Fun ‚îÄ‚îÄ‚îÄ
        non_empty_names = [n for n in self.names_collected if n]
        if non_empty_names:
            self._print("")
            self._print(self._c("purple", "    ‚îÄ‚îÄ‚îÄ Name Fun ‚îÄ‚îÄ‚îÄ"))
            longest = max(non_empty_names, key=len)
            shortest = min(non_empty_names, key=len)
            self._print(f"    Longest name:         {longest} ({len(longest)} chars)")
            self._print(f"    Shortest name:        {shortest} ({len(shortest)} chars)")

            # Most common starting letter
            if self.letters_seen:
                letter_counts = Counter(n[0].upper() for n in self.names_collected if n and n[0].isalpha())
                if letter_counts:
                    top_letter = letter_counts.most_common(1)[0]
                    self._print(f"    Most common letter:   {top_letter[0]} ({top_letter[1]} names)")

        # ‚îÄ‚îÄ‚îÄ Random Spotlight ‚îÄ‚îÄ‚îÄ
        if non_empty_names:
            self._print("")
            spotlight = random.choice(non_empty_names)
            self._print(self._c("yellow", f"    ‚≠ê Today's featured squish: {spotlight}!"))

        self._print("")

        # Show actual file paths
        if html_path:
            abs_html = os.path.abspath(html_path)
            self._print(self._c("green", "    Your Squishmallowdex is ready!"))
            self._print(self._c("cyan", f"    HTML: {abs_html}"))
        if csv_path:
            abs_csv = os.path.abspath(csv_path)
            self._print(self._c("cyan", f"    CSV:  {abs_csv}"))
        abs_log = os.path.abspath(self.log_file)
        self._print(self._c("gray", f"    Log:  {abs_log}\n"))

        # Easter egg: Show phoenix LAST when ALL squishmallows are collected!
        if self.show_phoenix_art or collected_all:
            self.show_phoenix()


# Global logger instance (set up in main)
log: AdventureLog = AdventureLog()

BASE = "https://squishmallowsquad.fandom.com"
MASTER_LIST = urljoin(BASE, "/wiki/Master_List")

HEADERS = {
    "User-Agent": "Squishmallowdex/1.0 (personal use)",
}

SKIP_NAMESPACES = (
    "File:",
    "Category:",
    "Special:",
    "Help:",
    "User:",
    "Template:",
    "Talk:",
)

NOISY_PAGES = re.compile(
    r"/wiki/(Master_List|Main_Page|All_Pages|Animals|Foods|Mythical_Creatures|By_.*)$"
)
NON_CHARACTER_TITLES = re.compile(
    r"\b(Master List|Main Page|All Pages|Squishville|Roblox)\b", re.I
)


def _sha1(s: str) -> str:
    # Hashing gives us stable, filesystem-safe filenames for cached pages and images.
    return hashlib.sha1(s.encode("utf-8")).hexdigest()


def cache_path(cache_dir: str, url: str) -> str:
    # Use the URL hash so each page maps to exactly one cached file.
    return os.path.join(cache_dir, f"{_sha1(url)}.html")


def fetch(
    url: str,
    session: requests.Session,
    *,
    cache_dir: str,
    refresh: bool,
    delay: float,
) -> bytes:
    """
    Our "polite fetcher" - downloads web pages carefully and saves them.

    How it works:
    1) Check if we already have this page saved (cache hit = fast!)
    2) If not, download it from the internet (cache miss = slower)
    3) Wait a bit before the next request (so we don't overwhelm the server)
    """
    os.makedirs(cache_dir, exist_ok=True)
    path = cache_path(cache_dir, url)

    # Check the cache first - like looking in your backpack before buying a new pencil!
    if os.path.exists(path) and not refresh:
        log.cache_hit(url.split("/")[-1])
        with open(path, "rb") as f:
            return f.read()

    # Cache miss - we need to download from the internet
    log.cache_miss(url.split("/")[-1])
    log.debug(f"Full URL: {url}")

    # Network call: keep a short timeout so we don't hang forever.
    resp = session.get(url, headers=HEADERS, timeout=30)
    resp.raise_for_status()
    content = resp.content

    # Save to cache for next time
    with open(path, "wb") as f:
        f.write(content)
    log.debug(f"Saved {len(content):,} bytes to cache")

    # Slow down between requests so we don't hammer the site.
    # This is called "rate limiting" - it's the polite thing to do!
    time.sleep(delay)
    return content


def extract_master_list_urls(master_html: bytes) -> list[str]:
    # Parse the Master List and collect all /wiki/... links.
    soup = BeautifulSoup(master_html, "html.parser")
    content = soup.select_one("div.mw-parser-output") or soup

    urls: list[str] = []
    seen: set[str] = set()

    # Each Squishmallow page is usually linked from the Master List.
    for a in content.select("a[href^='/wiki/']"):
        href = (a.get("href") or "").split("#")[0]
        if not href.startswith("/wiki/"):
            continue

        title_part = href.replace("/wiki/", "", 1)
        # Skip non-article namespaces like File:, Category:, etc.
        if any(title_part.startswith(ns) for ns in SKIP_NAMESPACES):
            continue

        full = urljoin(BASE, href)
        if full not in seen:
            seen.add(full)
            urls.append(full)

    # Filter obvious list/index pages that are not individual characters.
    urls = [u for u in urls if not NOISY_PAGES.search(u)]
    return urls


def normalize_label(label: str) -> str:
    # Infobox labels sometimes have messy whitespace; normalize it.
    return re.sub(r"\s+", " ", label.strip())


def parse_infobox(soup: BeautifulSoup) -> dict[str, str]:
    # The wiki uses a "portable infobox" with label/value pairs.
    info: dict[str, str] = {}
    aside = soup.select_one("aside.portable-infobox")
    if not aside:
        return info

    for item in aside.select(".pi-data"):
        lab_el = item.select_one(".pi-data-label")
        val_el = item.select_one(".pi-data-value")
        if not lab_el or not val_el:
            continue

        # Labels are things like "Type", "Color", "Squad", etc.
        label = normalize_label(lab_el.get_text(" ", strip=True))
        links = [
            a.get_text(" ", strip=True)
            for a in val_el.select("a")
            if a.get_text(strip=True)
        ]
        # Prefer linked values (cleaner) but fall back to raw text when needed.
        value = ", ".join(dict.fromkeys(links)) if links else val_el.get_text(" ", strip=True)

        if label and value:
            info[label] = value

    return info


def section_text_after_heading(
    soup: BeautifulSoup, heading_text: str, max_chars: int = 2500
) -> str | None:
    # Find the first heading like "Bio" and collect text until the next heading.
    headline = None
    for span in soup.select("span.mw-headline"):
        if span.get_text(strip=True) == heading_text:
            headline = span
            break
    if not headline:
        return None

    heading = headline.parent
    parts: list[str] = []

    # Walk forward through SIBLINGS only (not descendants) to avoid duplicates.
    for sib in heading.find_next_siblings():
        if sib.name in ("h2", "h3", "h4") and sib.find("span", class_="mw-headline"):
            break
        if sib.name in ("p", "blockquote", "ul", "ol", "div"):
            txt = " ".join(sib.stripped_strings)
            if txt:
                parts.append(txt)
        # Stop if the bio is getting huge; we want readable snippets.
        if sum(len(p) for p in parts) > max_chars:
            break

    text = " ".join(parts).strip()
    return text or None


def parse_squish_page(
    html: bytes, url: str
) -> tuple[dict[str, str | None], dict[str, str]]:
    # Parse a single Squishmallow page into a row dict + full infobox dict.
    soup = BeautifulSoup(html, "html.parser")

    # 1) Name: prefer the visible H1 title, fallback to OpenGraph title.
    name = None
    h1 = soup.find("h1")
    if h1:
        name = h1.get_text(" ", strip=True)
    if not name:
        ogt = soup.find("meta", property="og:title")
        if ogt and ogt.get("content"):
            name = ogt["content"].split("|")[0].strip()

    # 2) Image: OpenGraph image is the most reliable thumbnail.
    image = None
    ogi = soup.find("meta", property="og:image")
    if ogi and ogi.get("content"):
        image = ogi["content"]

    # 3) Infobox: structured stats we can keep in columns.
    info = parse_infobox(soup)

    # These are the fields we want to elevate into their own columns.
    wanted = [
        "Type",
        "Color",
        "Squad",
        "Size(s)",
        "Collector Number",
        "Year",
    ]
    row: dict[str, str | None] = {
        "Name": name,
        "URL": url,
        "ImageURL": image,
        "Bio": section_text_after_heading(soup, "Bio"),
    }

    for k in wanted:
        row[k] = info.get(k)

    # Preserve any remaining infobox keys so nothing is lost.
    extras = {k: v for k, v in info.items() if k not in wanted}
    row["Extra"] = json.dumps(extras, ensure_ascii=False) if extras else None
    return row, info


def is_character_page(name: str | None, info: dict[str, str]) -> bool:
    # Heuristic filter: we only keep pages that look like character profiles.
    if not name:
        return False
    if NON_CHARACTER_TITLES.search(name):
        return False
    if not info:
        return False
    required_keys = {
        "Type",
        "Color",
        "Squad",
        "Size(s)",
        "Collector Number",
        "Year",
    }
    # A page without any of these keys is probably a list/meta page.
    if not any(k in info for k in required_keys):
        return False
    return True


def download_image(
    session: requests.Session, url: str, out_dir: str, refresh: bool
) -> str | None:
    # Download and cache a thumbnail so the HTML page works offline.
    if not url:
        return None
    os.makedirs(out_dir, exist_ok=True)

    ext = os.path.splitext(urlparse(url).path)[1].lower()
    if ext not in (".jpg", ".jpeg", ".png", ".webp", ".gif"):
        ext = ".jpg"

    path = os.path.join(out_dir, f"{_sha1(url)}{ext}")
    if os.path.exists(path) and not refresh:
        return path

    # Image fetch: keep timeout short so a single broken image doesn't stall.
    resp = session.get(url, headers=HEADERS, timeout=30)
    resp.raise_for_status()
    content = resp.content

    # Validate we got the full image (protect against truncated downloads)
    expected_len = resp.headers.get("Content-Length")
    if expected_len:
        try:
            if len(content) < int(expected_len):
                log.warn(f"Incomplete image download: got {len(content)} of {expected_len} bytes")
                return None
        except ValueError:
            pass  # Invalid Content-Length header, skip validation

    # Basic sanity check: images should have some content
    if len(content) < 100:
        log.warn(f"Suspiciously small image ({len(content)} bytes), skipping")
        return None

    with open(path, "wb") as f:
        f.write(content)
    return path


def render_html(rows: list[dict[str, str | None]], out_path: str, title: str, logo_path: str = "") -> None:
    # Render a self-contained HTML page with search, sorting, and column picker.
    columns = [
        "Image",
        "Name",
        "Type",
        "Color",
        "Squad",
        "Size(s)",
        "Collector Number",
        "Year",
        "Bio",
        "Page",
    ]
    # Columns hidden by default
    hidden_default = {"Squad", "Size(s)", "Collector Number"}
    # Columns that shouldn't be sortable
    no_sort = {"Image", "Page", "Bio"}

    # Build header with data attributes for sorting/hiding
    head_cells = []
    for i, col in enumerate(columns):
        sortable = "true" if col not in no_sort else "false"
        hidden = "true" if col in hidden_default else "false"
        head_cells.append(f'<th data-col="{i}" data-sortable="{sortable}" data-hidden="{hidden}">{escape(col)}</th>')
    head = f"<tr>{''.join(head_cells)}</tr>"

    # Build the table body rows with data attributes
    body_rows = []
    for row in rows:
        cells = []
        for i, col in enumerate(columns):
            val = row.get(col) or ""
            cells.append(f'<td data-col="{i}">{val}</td>')
        body_rows.append(f"<tr>{''.join(cells)}</tr>")

    # Load and encode logo if available
    logo_b64 = ""
    if logo_path and os.path.exists(logo_path):
        with open(logo_path, "rb") as f:
            logo_b64 = base64.b64encode(f.read()).decode("ascii")

    logo_html = f'<img src="data:image/png;base64,{logo_b64}" alt="Squishmallowdex" class="logo"/>' if logo_b64 else ""

    # Column config as JSON for JS
    col_config = json.dumps([{"name": c, "hidden": c in hidden_default, "sortable": c not in no_sort} for c in columns])

    # Inline CSS/JS so the page works fully offline.
    html = f"""<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>{escape(title)}</title>
<style>
  :root {{
    --ink: #1f1a16;
    --accent: #ff7a59;
    --card: #ffffff;
    --muted: #6b5b52;
  }}
  * {{ box-sizing: border-box; }}
  body {{
    margin: 0;
    font-family: "Trebuchet MS", "Verdana", "Geneva", sans-serif;
    color: var(--ink);
    background: #ffffff;
  }}
  header {{
    padding: 24px 20px 16px;
    display: flex;
    flex-wrap: wrap;
    align-items: center;
    gap: 16px;
  }}
  .logo {{
    height: 64px;
    width: auto;
    border-radius: 12px;
    background: #fff;
    padding: 4px;
  }}
  .header-text {{
    flex: 1;
    min-width: 200px;
  }}
  h1 {{
    margin: 0 0 4px;
    font-size: 28px;
    letter-spacing: 0.5px;
  }}
  .sub {{
    margin: 0;
    color: var(--muted);
    font-size: 14px;
  }}
  .controls {{
    display: flex;
    gap: 10px;
    align-items: center;
    flex-wrap: wrap;
    width: 100%;
  }}
  .search {{
    flex: 1;
    min-width: 200px;
    max-width: 400px;
    padding: 10px 12px;
    border: 2px solid var(--accent);
    border-radius: 12px;
    font-size: 16px;
    background: var(--card);
  }}
  .col-picker {{
    position: relative;
  }}
  .col-btn {{
    padding: 10px 14px;
    border: 2px solid #ddd;
    border-radius: 12px;
    background: var(--card);
    cursor: pointer;
    font-size: 14px;
    display: flex;
    align-items: center;
    gap: 6px;
  }}
  .col-btn:hover {{
    border-color: var(--accent);
  }}
  .col-dropdown {{
    display: none;
    position: absolute;
    top: 100%;
    right: 0;
    margin-top: 4px;
    background: var(--card);
    border: 1px solid #ddd;
    border-radius: 12px;
    box-shadow: 0 8px 24px rgba(0,0,0,0.12);
    padding: 8px 0;
    min-width: 180px;
    z-index: 100;
  }}
  .col-dropdown.open {{
    display: block;
  }}
  .col-dropdown label {{
    display: flex;
    align-items: center;
    gap: 8px;
    padding: 8px 14px;
    cursor: pointer;
    font-size: 14px;
  }}
  .col-dropdown label:hover {{
    background: #fff7ef;
  }}
  .col-dropdown input {{
    width: 16px;
    height: 16px;
    accent-color: var(--accent);
  }}
  .table-wrap {{
    padding: 0 16px 28px;
    overflow-x: auto;
  }}
  table {{
    width: 100%;
    border-collapse: collapse;
    background: var(--card);
    border-radius: 14px;
    overflow: hidden;
    box-shadow: 0 8px 24px rgba(0, 0, 0, 0.08);
  }}
  th, td {{
    padding: 10px 12px;
    text-align: left;
    vertical-align: top;
    border-bottom: 1px solid #f0e5dc;
  }}
  th {{
    background: #fff1e5;
    position: sticky;
    top: 0;
    z-index: 1;
    white-space: nowrap;
  }}
  th[data-sortable="true"] {{
    cursor: pointer;
    user-select: none;
  }}
  th[data-sortable="true"]:hover {{
    background: #ffe8d5;
  }}
  th .sort-indicator {{
    margin-left: 4px;
    opacity: 0.3;
  }}
  th.sorted .sort-indicator {{
    opacity: 1;
  }}
  tr:hover {{
    background: #fff7ef;
  }}
  img.thumb {{
    width: 72px;
    height: 72px;
    object-fit: cover;
    border-radius: 12px;
    border: 2px solid #ffe1d0;
  }}
  /* Bio column: limit width for comfortable reading */
  td[data-col="8"] {{
    max-width: 300px;
    line-height: 1.4;
  }}
  .page-link {{
    color: var(--accent);
    text-decoration: none;
    font-weight: 600;
  }}
  .hidden {{
    display: none !important;
  }}
  .count {{
    color: var(--muted);
    font-size: 13px;
    padding: 8px 16px;
  }}
</style>
</head>
<body>
<header>
  {logo_html}
  <div class="header-text">
    <h1>{escape(title)}</h1>
    <p class="sub">Your Pok&#233;dex for Squishmallows. Works offline!</p>
  </div>
  <div class="controls">
    <input id="search" class="search" type="search" placeholder="Search by name, type, color..."/>
    <div class="col-picker">
      <button class="col-btn" id="colBtn">‚öôÔ∏è Columns</button>
      <div class="col-dropdown" id="colDropdown"></div>
    </div>
  </div>
</header>
<div class="count" id="count"></div>
<div class="table-wrap">
  <table id="dex">
    <thead>{head}</thead>
    <tbody>
      {''.join(body_rows)}
    </tbody>
  </table>
</div>
<script>
(function() {{
  const colConfig = {col_config};
  const table = document.getElementById('dex');
  const tbody = table.querySelector('tbody');
  const headers = Array.from(table.querySelectorAll('th'));
  const search = document.getElementById('search');
  const colBtn = document.getElementById('colBtn');
  const colDropdown = document.getElementById('colDropdown');
  const countEl = document.getElementById('count');

  // Load saved column visibility
  let visibility = {{}};
  try {{
    visibility = JSON.parse(localStorage.getItem('squishdex-cols') || '{{}}');
  }} catch(e) {{}}

  // Initialize visibility from config/storage
  colConfig.forEach((c, i) => {{
    if (visibility[c.name] === undefined) {{
      visibility[c.name] = !c.hidden;
    }}
  }});

  // Build column picker
  colConfig.forEach((c, i) => {{
    const label = document.createElement('label');
    const cb = document.createElement('input');
    cb.type = 'checkbox';
    cb.checked = visibility[c.name];
    cb.addEventListener('change', () => {{
      visibility[c.name] = cb.checked;
      localStorage.setItem('squishdex-cols', JSON.stringify(visibility));
      applyVisibility();
    }});
    label.appendChild(cb);
    label.appendChild(document.createTextNode(c.name));
    colDropdown.appendChild(label);
  }});

  // Toggle dropdown
  colBtn.addEventListener('click', (e) => {{
    e.stopPropagation();
    colDropdown.classList.toggle('open');
  }});
  document.addEventListener('click', () => {{
    colDropdown.classList.remove('open');
  }});
  colDropdown.addEventListener('click', (e) => {{
    e.stopPropagation();
  }});

  // Apply column visibility
  function applyVisibility() {{
    colConfig.forEach((c, i) => {{
      const show = visibility[c.name];
      table.querySelectorAll(`[data-col="${{i}}"]`).forEach(el => {{
        el.classList.toggle('hidden', !show);
      }});
    }});
  }}
  applyVisibility();

  // Sorting
  let sortCol = -1;
  let sortDir = 1;

  headers.forEach((th, i) => {{
    if (th.dataset.sortable === 'true') {{
      const indicator = document.createElement('span');
      indicator.className = 'sort-indicator';
      indicator.textContent = '‚Üï';
      th.appendChild(indicator);

      th.addEventListener('click', () => {{
        if (sortCol === i) {{
          sortDir *= -1;
        }} else {{
          sortCol = i;
          sortDir = 1;
        }}
        sortTable();
        headers.forEach(h => h.classList.remove('sorted'));
        th.classList.add('sorted');
        indicator.textContent = sortDir === 1 ? '‚Üë' : '‚Üì';
      }});
    }}
  }});

  function sortTable() {{
    const rows = Array.from(tbody.querySelectorAll('tr'));
    rows.sort((a, b) => {{
      const aVal = a.children[sortCol]?.textContent || '';
      const bVal = b.children[sortCol]?.textContent || '';
      // Try numeric sort first
      const aNum = parseFloat(aVal);
      const bNum = parseFloat(bVal);
      if (!isNaN(aNum) && !isNaN(bNum)) {{
        return (aNum - bNum) * sortDir;
      }}
      return aVal.localeCompare(bVal) * sortDir;
    }});
    rows.forEach(r => tbody.appendChild(r));
  }}

  // Search
  function updateCount() {{
    const visible = tbody.querySelectorAll('tr:not([style*="display: none"])').length;
    const total = tbody.querySelectorAll('tr').length;
    countEl.textContent = visible === total ? `${{total}} Squishmallows` : `Showing ${{visible}} of ${{total}}`;
  }}

  search.addEventListener('input', () => {{
    const q = search.value.trim().toLowerCase();
    Array.from(tbody.querySelectorAll('tr')).forEach(row => {{
      const text = row.textContent.toLowerCase();
      row.style.display = text.includes(q) ? '' : 'none';
    }});
    updateCount();
  }});

  updateCount();
}})();
</script>
</body>
</html>
"""

    with open(out_path, "w", encoding="utf-8") as f:
        f.write(html)


def build_html_rows(
    rows: list[dict[str, str | None]],
    session: requests.Session,
    *,
    download_images: bool,
    images_dir: str,
    refresh: bool,
) -> list[dict[str, str | None]]:
    # Convert raw rows into HTML-ready rows (adds <img> and link columns).
    html_rows: list[dict[str, str | None]] = []
    for row in rows:
        local_img = None
        if download_images:
            local_img = download_image(
                session, row.get("ImageURL") or "", images_dir, refresh
            )

        img_src = local_img or row.get("ImageURL") or ""
        full_img = row.get("ImageURL") or ""
        row_html = dict(row)
        # Make image clickable to open full-size version
        if img_src:
            img_tag = f'<img class="thumb" src="{escape(img_src)}" loading="lazy"/>'
            if full_img:
                row_html["Image"] = f'<a href="{escape(full_img)}" target="_blank" rel="noopener">{img_tag}</a>'
            else:
                row_html["Image"] = img_tag
        else:
            row_html["Image"] = ""
        row_html["Page"] = (
            f'<a class="page-link" href="{escape(row.get("URL") or "")}" target="_blank" rel="noopener">wiki</a>'
            if row.get("URL")
            else ""
        )
        html_rows.append(row_html)

    return html_rows


def write_csv(rows: list[dict[str, str | None]], out_path: str) -> None:
    # CSV is "portable": you can open it in spreadsheets or use it in other tools.
    # Write a spreadsheet-friendly CSV with all raw fields.
    with open(out_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=[
                "Name",
                "URL",
                "ImageURL",
                "Type",
                "Color",
                "Squad",
                "Size(s)",
                "Collector Number",
                "Year",
                "Bio",
                "Extra",
            ],
            extrasaction="ignore",  # Ignore old fields like Squishdate
        )
        writer.writeheader()
        for row in rows:
            writer.writerow(row)


def read_existing_csv(csv_path: str) -> list[dict[str, str | None]]:
    # If a previous run exists, load it so we can keep building on it.
    if not os.path.exists(csv_path):
        return []
    rows: list[dict[str, str | None]] = []
    with open(csv_path, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(dict(row))
    return rows


def read_progress(progress_path: str) -> set[str]:
    # The progress file stores one URL per line.
    if not os.path.exists(progress_path):
        return set()
    with open(progress_path, "r", encoding="utf-8") as f:
        return {line.strip() for line in f if line.strip()}


def append_progress(progress_path: str, url: str) -> None:
    # Append a single URL so we can resume without repeating work.
    with open(progress_path, "a", encoding="utf-8") as f:
        f.write(url + "\n")


def save_collection(
    rows: list[dict[str, str | None]],
    session: requests.Session,
    *,
    html_path: str,
    csv_path: str,
    images_dir: str,
    download_images: bool,
    refresh: bool,
) -> None:
    """Save the collection to HTML and CSV files."""
    html_rows = build_html_rows(
        rows,
        session,
        download_images=download_images,
        images_dir=images_dir,
        refresh=refresh,
    )
    render_html(html_rows, html_path, "Squishmallowdex", logo_path=LOGO_PATH)
    write_csv(rows, csv_path)


def main() -> None:
    global log

    ap = argparse.ArgumentParser(
        description=(
            "‚ú® SQUISHMALLOWDEX ‚ú®\n"
            "Build your own searchable Squishmallow database!\n"
            "Like a Pok√©dex, but for Squishmallows. Works offline too!"
        ),
        epilog=(
            "üåü QUICK START EXAMPLES üåü\n"
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
            "  Catch your first 50 Squishmallows (with images!):\n"
            "    python squishmallowdex.py --limit 50\n"
            "\n"
            "  Faster run (skip image downloads):\n"
            "    python squishmallowdex.py --limit 50 --no-download-images\n"
            "\n"
            "  See downloads and network activity:\n"
            "    python squishmallowdex.py -v\n"
            "\n"
            "  Full debug mode:\n"
            "    python squishmallowdex.py -vv\n"
            "\n"
            "  Just check your collection stats:\n"
            "    python squishmallowdex.py --stats-only\n"
            "\n"
            "üí° TIP: You can stop anytime with Ctrl+C and resume later!\n"
            "        Your progress is saved after every batch.\n"
        ),
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    # === Collection Options ===
    collect = ap.add_argument_group("‚ú® Collection Options")
    collect.add_argument(
        "--limit",
        type=int,
        default=0,
        metavar="N",
        help="How many NEW Squishmallows to catch this run (0 = catch 'em all!).",
    )
    collect.add_argument(
        "--batch-size",
        type=int,
        default=10,
        metavar="N",
        help="Save progress every N catches (default: 10).",
    )
    collect.add_argument(
        "--no-download-images",
        action="store_true",
        help="Skip downloading pictures (faster, uses less space).",
    )
    collect.add_argument(
        "--stats-only",
        action="store_true",
        help="Just show collection stats, don't download anything new.",
    )

    # === Output Options ===
    output = ap.add_argument_group("üìÅ Output Files")
    output.add_argument(
        "--out",
        default="squishmallowdex.html",
        metavar="FILE",
        help="HTML file to create (open in browser).",
    )
    output.add_argument(
        "--csv",
        default="squishmallowdex.csv",
        metavar="FILE",
        help="CSV file to create (open in spreadsheet).",
    )
    output.add_argument(
        "--images-dir",
        default="squish_images",
        metavar="DIR",
        help="Folder for downloaded images.",
    )

    # === Display Options ===
    display = ap.add_argument_group("üé® Display Options")
    display.add_argument(
        "-v", "--verbose",
        action="count",
        default=0,
        help="Show more details (-v = downloads/skips, -vv = debug).",
    )
    display.add_argument(
        "--no-adventure",
        action="store_true",
        help="Turn off fun facts and tips.",
    )
    display.add_argument(
        "--no-color",
        action="store_true",
        help="Turn off colors (for terminals that don't support them).",
    )
    display.add_argument(
        "-q", "--quiet",
        action="store_true",
        help="Minimal output (just errors and final summary).",
    )
    display.add_argument(
        "--log",
        default="squishmallow.log",
        metavar="FILE",
        help="Log file to write all output (default: squishmallow.log).",
    )
    display.add_argument(
        "--phoenix",
        action="store_true",
        help=argparse.SUPPRESS,  # Hidden easter egg!
    )

    # === Advanced Options ===
    advanced = ap.add_argument_group("‚öôÔ∏è  Advanced Options (for power users!)")
    advanced.add_argument(
        "--delay",
        type=float,
        default=1.2,
        metavar="SEC",
        help="Wait time between downloads (be nice to servers!).",
    )
    advanced.add_argument(
        "--batch-delay",
        type=float,
        default=5.0,
        metavar="SEC",
        help="Extra wait time between batches.",
    )
    advanced.add_argument(
        "--cache",
        default="cache_html",
        metavar="DIR",
        help="Folder for cached web pages.",
    )
    advanced.add_argument(
        "--progress-file",
        default="progress_urls.txt",
        metavar="FILE",
        help="File that tracks which pages we've visited.",
    )
    advanced.add_argument(
        "--refresh",
        action="store_true",
        help="Re-download everything (ignores cache).",
    )
    advanced.add_argument(
        "--rebuild",
        action="store_true",
        help="Rebuild collection from cache (re-process all cached pages).",
    )

    args = ap.parse_args()

    # Set up the adventure logger with user's preferences
    log = AdventureLog(
        verbose=0 if args.quiet else args.verbose,
        adventure_mode=not args.no_adventure,
        use_color=not args.no_color and sys.stdout.isatty(),
        quiet=args.quiet,
        show_phoenix_art=args.phoenix,
        start_time=time.time(),
        log_file=args.log,
    )

    # Open log file for this session
    log.open_log()

    # Show the awesome banner!
    log.banner()

    # A Session keeps connections open between requests (faster + nicer).
    session = requests.Session()

    # Handle --rebuild: clear progress to re-process all cached pages
    if args.rebuild:
        log.info("Rebuild mode: clearing progress files to re-process from cache...", 0)
        skipped_file = skipped_file_path(args.progress_file)
        for f in [args.progress_file, skipped_file]:
            if os.path.exists(f):
                os.remove(f)

    # Load any existing CSV so we can resume without losing progress.
    # In rebuild mode, we start fresh.
    rows: list[dict[str, str | None]] = [] if args.rebuild else read_existing_csv(args.csv)
    existing_count = len(rows)

    # Track stats for existing rows
    for row in rows:
        log.track_squish(row)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # STATS-ONLY MODE: Just show what we have and exit
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if args.stats_only:
        log.step("Checking your collection...")
        if not rows:
            log.warn("No Squishmallows collected yet! Run without --stats-only to start catching.")
        else:
            log.info(f"Found {len(rows)} Squishmallows in your collection!")
            log.summary(len(rows), existing_count, csv_path=args.csv, html_path=args.out)
        log.close_log()
        return

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # COLLECTION MODE: Let's catch some Squishmallows!
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    log.step("Step 1: Fetching the Master List of all Squishmallows...")

    if log.adventure_mode:
        log.info("The Master List is like a treasure map - it shows us where all the Squishmallows are!", 0)

    master_html = fetch(
        MASTER_LIST, session, cache_dir=args.cache, refresh=args.refresh, delay=args.delay
    )
    urls = extract_master_list_urls(master_html)

    log.info(f"Found {len(urls)} potential Squishmallow pages to explore!", 0)

    # The progress file tracks which URLs we've already finished so we can skip them.
    processed_urls = read_progress(args.progress_file)
    skipped_file = skipped_file_path(args.progress_file)
    skipped_urls = read_progress(skipped_file)

    # If we have prior rows but no progress file, rebuild progress from CSV.
    if rows and not processed_urls:
        log.info("Rebuilding progress from existing CSV...", 1)
        processed_urls = {r.get("URL", "") for r in rows if r.get("URL")}
        # Write all at once instead of appending one by one (avoids duplicates)
        with open(args.progress_file, "w", encoding="utf-8") as f:
            for url in sorted(processed_urls):
                f.write(url + "\n")

    if existing_count > 0:
        log.info(f"Resuming collection! You already have {existing_count} Squishmallows.", 0)

    log.step("Step 2: Hunting for Squishmallows!")

    if log.adventure_mode:
        log.info("Each page we visit might contain a new friend for your collection!", 0)

    collected_in_batch = 0
    batch_number = 1
    limit = args.limit if args.limit and args.limit > 0 else None

    # Count how many we need to process
    urls_to_process = [u for u in urls if u not in processed_urls and u not in skipped_urls]
    if limit:
        # Limit means "process this many more", not "have this many total"
        remaining_to_catch = min(limit, len(urls_to_process))
    else:
        remaining_to_catch = len(urls_to_process)

    if remaining_to_catch <= 0:
        log.info("Your collection is already complete! Nothing new to catch.", 0)
    else:
        log.info(f"About to hunt for up to {remaining_to_catch} new Squishmallows...", 0)

    for u in urls:
        # Skip already processed or known non-character pages
        if u in processed_urls or u in skipped_urls:
            continue
        if limit is not None and log.new_catches >= limit:
            log.info(f"Reached your limit of {limit} new Squishmallows!", 0)
            break

        try:
            # Fetch and parse a single page.
            page_html = fetch(
                u, session, cache_dir=args.cache, refresh=args.refresh, delay=args.delay
            )
            row, info = parse_squish_page(page_html, u)

            # Skip pages that look like lists or meta pages.
            if not is_character_page(row.get("Name"), info):
                log.skip(row.get("Name") or u.split("/")[-1], "not a character page")
                # FIX: Record skipped URLs so we don't re-parse them next time
                skipped_urls.add(u)
                append_progress(skipped_file, u)
                continue

            rows.append(row)
            log.track_squish(row)
            processed_urls.add(u)
            append_progress(args.progress_file, u)
            collected_in_batch += 1

            # Celebrate the catch!
            log.catch(row.get("Name") or "Unknown", len(rows))

            # When we hit a batch boundary, write outputs and pause briefly.
            if collected_in_batch >= args.batch_size:
                log.step(f"Batch {batch_number} complete! Saving your progress...")
                save_collection(
                    rows, session,
                    html_path=args.out, csv_path=args.csv,
                    images_dir=args.images_dir,
                    download_images=not args.no_download_images,
                    refresh=args.refresh,
                )
                log.info(f"Saved! {len(rows)} total Squishmallows in {args.out}", 0)

                if log.adventure_mode:
                    log.info("Taking a short break to be nice to the server... üçµ", 0)

                batch_number += 1
                collected_in_batch = 0
                time.sleep(args.batch_delay)

        except KeyboardInterrupt:
            log.warn("Stopping early! Don't worry, your progress is saved.")
            break
        except Exception as exc:
            log.error(f"Problem with {u.split('/')[-1]}: {exc}")
            log.debug(f"Full error: {type(exc).__name__}: {exc}")

    # Final write for any leftover rows (last partial batch).
    log.step("Step 3: Saving your Squishmallowdex!")

    save_collection(
        rows, session,
        html_path=args.out, csv_path=args.csv,
        images_dir=args.images_dir,
        download_images=not args.no_download_images,
        refresh=args.refresh,
    )

    log.debug(f"Wrote HTML to {args.out}")
    log.debug(f"Wrote CSV to {args.csv}")

    # Show the epic summary!
    # Pass total_available=len(urls) for the phoenix easter egg (shows when ALL collected)
    log.summary(len(rows), existing_count, total_available=len(urls),
                csv_path=args.csv, html_path=args.out)

    # Close the log file
    log.close_log()


if __name__ == "__main__":
    main()
